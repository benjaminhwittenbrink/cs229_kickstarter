{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd25ace6-ac02-49f9-ba36-1a6a2e729756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7538fa40-e3ae-42ac-a8f2-5cde07ab4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1efa01a-1ef2-4b87-899c-245312d8bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "013421a1-c0b9-47e9-b508-67a96d0f613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/all_processed_df.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a10e3c-b79d-47a0-8568-f5d7ab3b7d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    132754\n",
       "0     88494\n",
       "Name: outcome, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['outcome'] = (df['state']=='successful').astype(int)\n",
    "df.outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "173a3284-605b-414f-aca7-70b9ab796bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_replace_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "RE_symbols_to_drop = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(txt):\n",
    "    if txt is None: return ''\n",
    "    txt = str(txt)\n",
    "    txt = txt.lower()\n",
    "    txt = RE_replace_space.sub(' ', txt)\n",
    "    txt = RE_symbols_to_drop.sub('', txt)\n",
    "    txt = ' '.join(word for word in txt.split() if word not in STOPWORDS)\n",
    "    return txt \n",
    "\n",
    "df['blurb_cln'] = df['blurb'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b8cd08-f71f-4538-9967-398bf6064e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, df['outcome'], random_state=229, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7602c018-59cd-4a70-bab5-2ea9346ba065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       humble little astro traveling around brewing f...\n",
       "1       spooky ghost miniatures tabletop gaming rpg dn...\n",
       "2       part dreamcatcher comics starting gaming depar...\n",
       "3       write dd gaming modules make game mastering ex...\n",
       "4       wander creates high quality handcrafted functi...\n",
       "                              ...                        \n",
       "1628    david griswold eliza reisfeld proud present fi...\n",
       "1633    jeremy clark one toptier traditional comic boo...\n",
       "1640    hugs bugs cleverly written illustrated moral t...\n",
       "1662    biker club formed ashes apocalypse ldmc fights...\n",
       "1736                                 luxury playing cards\n",
       "Name: blurb_cln, Length: 221248, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['blurb_cln']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "840f3c2f-caa2-41ce-95d5-ad18f88d6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb2c5ae-7110-4bb5-968b-b31bdf5362f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = X_train.apply( lambda r: tokenize_text(r['blurb_cln']), axis=1).values\n",
    "test_tokenized = X_test.apply( lambda r: tokenize_text(r['blurb_cln']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac1bbfe8-3c8f-486c-b330-30d5bd46fef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benji/anaconda3/envs/play/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "161d1ecf-5a29-4ad3-b408-2f7ea098c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "def get_corpus(words):\n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[review] for review in words]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    \n",
    "    return corpus, id2word, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd847e61-4f33-48a6-a949-cf69003eb8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, train_id2word, bigram_train = get_corpus(train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "707e7d50-5d57-4818-ad3a-906d636ea159",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
    "                       corpus=train_corpus,\n",
    "                       num_topics=n_topics,\n",
    "                       id2word=train_id2word,\n",
    "                       chunksize=100,\n",
    "                       workers=7, # Num. Processing Cores - 1\n",
    "                       passes=50,\n",
    "                       eval_every = 1,\n",
    "                       per_word_topics=True, \n",
    "    random_state=229\n",
    ")\n",
    "#lda_train.save(\"data/lda/lda_train20.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5966087-a47f-4031-816c-351ffc80c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics=20\n",
    "lda_train = gensim.models.LdaModel.load(\"data/lda/lda_train20.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86f72f08-8885-454f-98d1-50a7a3b023fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"series\" + 0.013*\"set\" + 0.012*\"inspired\" + 0.011*\"two\" + 0.009*\"short_film\" + 0.009*\"enamel_pins\" + 0.009*\"collection\" + 0.008*\"story\" + 0.008*\"film\" + 0.008*\"world\"'),\n",
       " (1,\n",
       "  '0.014*\"made\" + 0.010*\"unique\" + 0.010*\"using\" + 0.009*\"design\" + 0.009*\"en\" + 0.009*\"designed\" + 0.007*\"real\" + 0.007*\"hand\" + 0.006*\"handmade\" + 0.006*\"100\"'),\n",
       " (2,\n",
       "  '0.009*\"food\" + 0.009*\"home\" + 0.006*\"natural\" + 0.006*\"local\" + 0.005*\"place\" + 0.005*\"water\" + 0.005*\"light\" + 0.005*\"community\" + 0.005*\"using\" + 0.005*\"products\"'),\n",
       " (3,\n",
       "  '0.027*\"game\" + 0.011*\"new\" + 0.010*\"play\" + 0.009*\"games\" + 0.007*\"characters\" + 0.006*\"world\" + 0.006*\"worlds\" + 0.006*\"action\" + 0.006*\"meets\" + 0.005*\"based\"'),\n",
       " (4,\n",
       "  '0.021*\"art\" + 0.013*\"project\" + 0.012*\"artists\" + 0.012*\"new\" + 0.011*\"dance\" + 0.009*\"work\" + 0.009*\"music\" + 0.008*\"release\" + 0.008*\"show\" + 0.007*\"community\"'),\n",
       " (5,\n",
       "  '0.036*\"de\" + 0.017*\"la\" + 0.009*\"un\" + 0.008*\"jazz\" + 0.007*\"que\" + 0.007*\"et\" + 0.007*\"tool\" + 0.006*\"comes\" + 0.006*\"debut\" + 0.006*\"el\"'),\n",
       " (6,\n",
       "  '0.013*\"get\" + 0.013*\"make\" + 0.011*\"help\" + 0.010*\"art\" + 0.010*\"new\" + 0.008*\"need\" + 0.007*\"create\" + 0.007*\"book\" + 0.007*\"want\" + 0.007*\"business\"'),\n",
       " (7,\n",
       "  '0.013*\"book\" + 0.011*\"online\" + 0.009*\"people\" + 0.009*\"kids\" + 0.008*\"art\" + 0.008*\"cards\" + 0.008*\"experience\" + 0.007*\"stories\" + 0.007*\"children\" + 0.006*\"series\"'),\n",
       " (8,\n",
       "  '0.020*\"life\" + 0.015*\"time\" + 0.012*\"one\" + 0.012*\"love\" + 0.012*\"world\" + 0.010*\"story\" + 0.009*\"family\" + 0.008*\"journey\" + 0.008*\"book\" + 0.007*\"back\"'),\n",
       " (9,\n",
       "  '0.033*\"album\" + 0.032*\"new\" + 0.027*\"music\" + 0.016*\"help\" + 0.012*\"need_help\" + 0.011*\"first\" + 0.011*\"songs\" + 0.011*\"record\" + 0.009*\"pins\" + 0.009*\"ep\"')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train.print_topics(num_topics=n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "740b150d-bf1e-4303-9982-948affab47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(train_tokenized)):\n",
    "    top_topics = (\n",
    "        lda_train.get_document_topics(train_corpus[i],\n",
    "                                      minimum_probability=0.0)\n",
    "    )\n",
    "    topic_vec = [top_topics[i][1] for i in range(n_topics)]\n",
    "    train_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50161a47-3e1c-4196-a4f4-22dd35323242",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df = pd.DataFrame(train_vecs)\n",
    "lda_df.columns = [\"lda_df_topic\" + str(i) for i in lda_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "876ecb12-53de-47b0-8ebb-1158987e01dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d760748-a145-4a5b-8326-5280bbdbac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f817e60-7547-4543-89e0-9703eb287101",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "lda_df_scale = scaler.fit_transform(lda_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ed05533-e9f6-4dd9-8c80-94c184dd2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(class_weight='balanced').fit(lda_df, y_train)\n",
    "clf_scale = LogisticRegression(class_weight='balanced').fit(lda_df_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a015d49-66f0-469f-a3f5-5198ff8ebf5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.62      0.53     62066\n",
      "           1       0.68      0.53      0.60     92807\n",
      "\n",
      "    accuracy                           0.57    154873\n",
      "   macro avg       0.57      0.57      0.56    154873\n",
      "weighted avg       0.59      0.57      0.57    154873\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.62      0.53     62066\n",
      "           1       0.68      0.53      0.60     92807\n",
      "\n",
      "    accuracy                           0.57    154873\n",
      "   macro avg       0.57      0.57      0.56    154873\n",
      "weighted avg       0.59      0.57      0.57    154873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, clf.predict(lda_df)))\n",
    "print(classification_report(y_train, clf_scale.predict(lda_df_scale)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e6a38e5-030d-412d-8311-39377abfeca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.50      0.49     62066\n",
      "           1       0.66      0.64      0.65     92807\n",
      "\n",
      "    accuracy                           0.59    154873\n",
      "   macro avg       0.57      0.57      0.57    154873\n",
      "weighted avg       0.59      0.59      0.59    154873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier(\n",
    "    max_iter=1000,\n",
    "    tol=1e-3,\n",
    "    loss='log',\n",
    "    class_weight='balanced'\n",
    ").fit(lda_df, y_train)\n",
    "print(classification_report(y_train, sgd.predict(lda_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e328849a-5851-48af-b2f6-cc98771f5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(words):\n",
    "    bigram = bigrams(words)\n",
    "    bigram = [bigram[review] for review in words]\n",
    "    return bigram\n",
    "  \n",
    "bigram_test = get_bigram(test_tokenized)\n",
    "\n",
    "test_corpus = [train_id2word.doc2bow(text) for text in bigram_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "43ab7ae7-479a-4177-941b-3482ed8372bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs = []\n",
    "for i in range(len(test_tokenized)):\n",
    "    top_topics = (\n",
    "            lda_train.get_document_topics(test_corpus[i],\n",
    "                                          minimum_probability=0.0)\n",
    "    )\n",
    "    topic_vec = [top_topics[i][1] for i in range(n_topics)]\n",
    "    test_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a6e26dc-259e-4184-864f-efbc3e469619",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df_test = pd.DataFrame(test_vecs)\n",
    "lda_df_test.columns = [\"lda_df_topic\" + str(i) for i in lda_df_test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "320a24cb-f8e9-4bd7-8ed1-d5c84844a90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.62      0.53     26428\n",
      "           1       0.68      0.54      0.60     39947\n",
      "\n",
      "    accuracy                           0.57     66375\n",
      "   macro avg       0.57      0.58      0.57     66375\n",
      "weighted avg       0.60      0.57      0.57     66375\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.50      0.49     26428\n",
      "           1       0.66      0.65      0.65     39947\n",
      "\n",
      "    accuracy                           0.59     66375\n",
      "   macro avg       0.57      0.57      0.57     66375\n",
      "weighted avg       0.59      0.59      0.59     66375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf.predict(lda_df_test)))\n",
    "print(classification_report(y_test, sgd.predict(lda_df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81278ff2-1fe9-40ed-a059-f4abf4a32eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df.to_csv(\"data/lda_df.csv\")\n",
    "lda_df_test.to_csv(\"data/lda_df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c849834-fe55-467f-8ebb-3b534e638c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
